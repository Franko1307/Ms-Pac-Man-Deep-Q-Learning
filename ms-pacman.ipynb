{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación del entorno Ms-Pac-Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape # [altura, anchura, canales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, hay nueve acciones discretas disponibles, que corresponden a las nueve posiciones posibles del joystick (izquierda, derecha, arriba, abajo, centro, superior izquierda, etc.), y las observaciones son simplemente capturas de pantalla de la pantalla Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas imágenes son un poco grandes, por lo que crearemos una pequeña función de preprocesamiento que recortará la imagen y la reducirá a 88 × 80 píxeles, la convertirá a escala de grises y mejorará el contraste de la Sra. Pac-Man. Esto reducirá la cantidad de cálculos requeridos por el DQN y acelerará el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspacman_color = np.array([210, 164, 74]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rl\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # recortar y reducir tamaño\n",
    "    img = img.mean(axis=2) # a escala de grises\n",
    "    img[img==mspacman_color] = 0 # improve contrast\n",
    "    img = (img - 128) / 128 - 1 # normalize from -1. to 1.\n",
    "    return img.reshape(88, 80, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure preprocessing_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Observación original (160 × 210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Observación preprocesada (88 × 80 en escala de grises)\")\n",
    "plt.imshow(img.reshape(88, 80), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "save_fig(\"preprocessing_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a crear el DQN. Simplemente podría tomar un par estado-acción (s, a) como entrada y generar una estimación de\n",
    "el Q-Value Q (s, a) correspondiente, pero como las acciones son discretas, es más conveniente utilizar una red neuronal que tome solo un estado s como entrada y genere una estimación de Q-Value por acción. El DQN estará compuesto por tres capas convolucionales, seguidas por dos capas totalmente conectadas, incluida la capa de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagen](images/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como veremos, el algoritmo de entrenamiento que usaremos requiere dos DQN con la misma arquitectura (pero con diferentes parámetros): uno se usará para conducir a la Sra. Pac-Man durante el entrenamiento (el actor), y el otro observará al actor y Aprende de sus pruebas y errores (la crítica). A intervalos regulares copiaremos la crítica al actor. Ya que necesitamos dos DQN idénticos, crearemos una función q_network () para construirlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32, 64, 64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = [\"SAME\"] * 3\n",
    "conv_activation = [tf.nn.relu] * 3\n",
    "n_hidden_in = 64 * 11 * 10 # conv3 tiene 64 mapas de 11x10 cada uno\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n # 9 acciones discretas están disponibles\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state / 128.0 # scale pixel intensities to the [-1.0, 1.0] range.\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        for n_maps, kernel_size, strides, padding, activation in zip(\n",
    "                conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "                conv_paddings, conv_activation):\n",
    "            prev_layer = tf.layers.conv2d(\n",
    "                prev_layer, filters=n_maps, kernel_size=kernel_size,\n",
    "                strides=strides, padding=padding, activation=activation,\n",
    "                kernel_initializer=initializer)\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "        hidden = tf.layers.dense(last_conv_layer_flat, n_hidden,\n",
    "                                 activation=hidden_activation,\n",
    "                                 kernel_initializer=initializer)\n",
    "        outputs = tf.layers.dense(hidden, n_outputs,\n",
    "                                  kernel_initializer=initializer)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                       scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                              for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera parte de este código define los hiperparámetros de la arquitectura DQN. Luego, la función q_network () crea el DQN, tomando el estado X_state del entorno como entrada y el nombre del ámbito de la variable. Tenga en cuenta que solo usaremos una observación para representar el estado del entorno, ya que casi no hay un estado oculto (excepto por los objetos parpadeantes y las direcciones de los fantasmas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a crear el marcador de posición de entrada, los dos DQN y la operación para copiar el DQN crítico al DQN actor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width,\n",
    "                                            input_channels])\n",
    "online_q_values, online_vars = q_network(X_state, name=\"q_networks/online\")\n",
    "target_q_values, target_vars = q_network(X_state, name=\"q_networks/target\")\n",
    "\n",
    "copy_ops = [target_var.assign(online_vars[var_name])\n",
    "            for var_name, target_var in target_vars.items()]\n",
    "copy_online_to_target = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/conv2d/kernel:0': <tf.Variable 'q_networks/online/conv2d/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " '/conv2d/bias:0': <tf.Variable 'q_networks/online/conv2d/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " '/conv2d_1/kernel:0': <tf.Variable 'q_networks/online/conv2d_1/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>,\n",
       " '/conv2d_1/bias:0': <tf.Variable 'q_networks/online/conv2d_1/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " '/conv2d_2/kernel:0': <tf.Variable 'q_networks/online/conv2d_2/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " '/conv2d_2/bias:0': <tf.Variable 'q_networks/online/conv2d_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " '/dense/kernel:0': <tf.Variable 'q_networks/online/dense/kernel:0' shape=(7040, 512) dtype=float32_ref>,\n",
       " '/dense/bias:0': <tf.Variable 'q_networks/online/dense/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " '/dense_1/kernel:0': <tf.Variable 'q_networks/online/dense_1/kernel:0' shape=(512, 9) dtype=float32_ref>,\n",
       " '/dense_1/bias:0': <tf.Variable 'q_networks/online/dense_1/bias:0' shape=(9,) dtype=float32_ref>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrocedamos por un segundo: ahora tenemos dos DQN que son capaces de tomar un estado de entorno (es decir, una observación preprocesada) como entrada y salida de un valor Q estimado para cada acción posible en ese estado.\n",
    "\n",
    "Además, tenemos una operación llamada copy_critic_to_actor para copiar todas las variables entrenables del crítico DQN al actor DQN. Utilizamos la función tf.group () de TensorFlow para agrupar todas las operaciones de asignación en una sola operación conveniente. \n",
    "\n",
    "El actor DQN se puede usar para interpretar a la Sra. Pac-Man (inicialmente muy mal). \n",
    "Como se mencionó anteriormente, desea que explore el juego lo suficientemente a fondo, por lo que generalmente desea combinarlo con una política avariciosa u otra estrategia de exploración. Pero ¿qué pasa con la crítica DQN? ¿Cómo aprenderá a jugar el juego? \n",
    "\n",
    "La respuesta corta es que intentará que sus predicciones de Q-Value coincidan con los Q-Values estimados por el actor a través de su experiencia en el juego. Específicamente, dejaremos que el actor juegue por un tiempo, almacenando todas sus experiencias en una memoria de repetición.\n",
    "\n",
    "Cada memoria será un 5-tupla (estado, acción, próximo estado, recompensa, continuar), donde el elemento \"continuar\" será igual a 0.0 cuando el juego termine, o 1.0 de lo contrario. Luego, a intervalos regulares, muestrearemos un lote de memorias de la memoria de repetición, y estimaremos los valores Q de estas memorias. \n",
    "\n",
    "Finalmente, capacitaremos al crítico DQN para predecir estos valores Q usando técnicas de aprendizaje supervisado regularmente. Una vez cada pocas iteraciones de entrenamiento, copiaremos la crítica DQN al actor DQN. ¡Y listo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "momentum = 0.95\n",
    "\n",
    "with tf.variable_scope(\"train\"):\n",
    "    X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                            axis=1, keepdims=True)\n",
    "    error = tf.abs(y - q_value)\n",
    "    clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "    linear_error = 2 * (error - clipped_error)\n",
    "    loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTA\n",
    "\n",
    "La función ReplayMemory es opcional, pero muy recomendable. Sin ella, entrenaría al crítico DQN usando experiencias consecutivas que pueden estar muy relacionadas.\n",
    "\n",
    "Esto introduciría una gran cantidad de sesgo y ralentizaría la convergencia del algoritmo de entrenamiento.\n",
    "Mediante el uso de una memoria de reproducción, nos aseguramos de que las memorias suministradas al algoritmo de entrenamiento puedan estar bastante sin correlacionar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.buf = np.empty(shape=maxlen, dtype=np.object)\n",
    "        self.index = 0\n",
    "        self.length = 0\n",
    "        \n",
    "    def append(self, data):\n",
    "        self.buf[self.index] = data\n",
    "        self.length = min(self.length + 1, self.maxlen)\n",
    "        self.index = (self.index + 1) % self.maxlen\n",
    "    \n",
    "    def sample(self, batch_size, with_replacement=True):\n",
    "        if with_replacement:\n",
    "            indices = np.random.randint(self.length, size=batch_size) # faster\n",
    "        else:\n",
    "            indices = np.random.permutation(self.length)[:batch_size]\n",
    "        return self.buf[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory_size = 500000\n",
    "replay_memory = ReplayMemory(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for memory in replay_memory.sample(batch_size):\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitaremos al actor para explorar el juego. Usaremos la política ε-greedy, y gradualmente disminuiremos ε de 1.0 a 0.05, en 50,000 pasos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 50000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos a inicializar algunas variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100000 # total number of training steps\n",
    "training_start = 1000 # start training after 1,000 game iterations\n",
    "training_interval = 3 # run a training step every 3 game iterations\n",
    "save_steps = 50 # save the model every 50 training steps\n",
    "copy_steps = 25 # copy the critic to the actor every 25 training steps\n",
    "discount_rate = 0.95\n",
    "skip_start = 90 # skip the start of every game (it's just waiting time)\n",
    "batch_size = 50\n",
    "iteration = 0 # game iterations\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True # env needs to be reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn.ckpt\n",
      "Iteration 6279\tTraining step 1747/100000 (1.7)%\tLoss 1.512141\tMean Max-Q 1.960082   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fbef059c98d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Train the online DQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         _, loss_val = sess.run([training_op, loss], feed_dict={\n\u001b[0;32m---> 53\u001b[0;31m             X_state: X_state_val, X_action: X_action_val, y: y_val})\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Regularly copy the online DQN to the target DQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Escuela/2018-2/Redes.Neuronales/Ms-Pac-Man-Deep-Q-Learning/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Escuela/2018-2/Redes.Neuronales/Ms-Pac-Man-Deep-Q-Learning/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Escuela/2018-2/Redes.Neuronales/Ms-Pac-Man-Deep-Q-Learning/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Escuela/2018-2/Redes.Neuronales/Ms-Pac-Man-Deep-Q-Learning/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Escuela/2018-2/Redes.Neuronales/Ms-Pac-Man-Deep-Q-Learning/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Escuela/2018-2/Redes.Neuronales/Ms-Pac-Man-Deep-Q-Learning/python36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + \".index\"):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        print(\"\\rIteration {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:5f}\\tMean Max-Q {:5f}   \".format(\n",
    "            iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\")\n",
    "        if done: # game over, start again\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start): # skip the start of each game\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observation(obs)\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "\n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(obs)\n",
    "\n",
    "        # Let's memorize what happened\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "\n",
    "        # Compute statistics for tracking progress (not shown in the book)\n",
    "        total_max_q += q_values.max()\n",
    "        game_length += 1\n",
    "        if done:\n",
    "            mean_max_q = total_max_q / game_length\n",
    "            total_max_q = 0.0\n",
    "            game_length = 0\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # only train after warmup period and at regular intervals\n",
    "        \n",
    "        # Sample memories and use the target DQN to produce the target Q-Value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size))\n",
    "        next_q_values = target_q_values.eval(\n",
    "            feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "\n",
    "        # Train the online DQN\n",
    "        _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "            X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        # Regularly copy the online DQN to the target DQN\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "\n",
    "        # And save regularly\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
